{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Seismic Interpolation\n",
    "\n",
    "This notebook demonstrates how to preprocess the synthetic geophone and DAS data for training the seismic interpolation model. We'll go through the following steps:\n",
    "\n",
    "1. Load the combined geophone and DAS dataset\n",
    "2. Preprocess and normalize the data\n",
    "3. Create masked datasets for training (simulating missing geophone channels)\n",
    "4. Prepare windowed data samples for the model\n",
    "5. Split the data into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add the project root to path for imports\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.preprocessing.data_loader import normalize_data, create_masked_dataset, prepare_dataset_for_training, split_dataset\n",
    "from src.preprocessing.dataset import SeismicDataset, TransformerSeismicDataset\n",
    "from src.utils.logging_utils import setup_logging\n",
    "from src.utils.plot_utils import plot_seismic_gather, plot_masked_gather\n",
    "\n",
    "# Set up logging\n",
    "logger = setup_logging(level='INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Combined Dataset\n",
    "\n",
    "First, let's load the combined dataset of geophone and DAS data that we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined dataset\n",
    "dataset_path = \"../data/synthetic/processed/simulation1/combined_dataset.npy\"\n",
    "combined_data = np.load(dataset_path, allow_pickle=True).item()\n",
    "\n",
    "# Extract data\n",
    "times = combined_data[\"times\"]\n",
    "geophone_data = combined_data[\"geophone_data\"]\n",
    "das_data = combined_data[\"das_data\"]\n",
    "station_coords = combined_data[\"station_coordinates\"]\n",
    "metadata = combined_data[\"metadata\"]\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Loaded dataset with {metadata['n_geophones']} geophones and {metadata['n_das_channels']} DAS channels\")\n",
    "print(f\"Each trace has {len(times)} time samples, dt = {metadata['dt']} s\")\n",
    "print(f\"DAS gauge length: {metadata['gauge_length']} m, channel spacing: {metadata['channel_spacing']} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Raw Data\n",
    "\n",
    "Let's visualize the raw geophone and DAS data to confirm they're loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the geophone data\n",
    "plot_seismic_gather(geophone_data, title=\"Geophone Data\", xlabel=\"Time Sample\", ylabel=\"Channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the DAS data\n",
    "plot_seismic_gather(das_data, title=\"DAS Data\", xlabel=\"Time Sample\", ylabel=\"Channel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and Normalize Data\n",
    "\n",
    "Now, let's preprocess and normalize the data to prepare it for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data (channel-wise)\n",
    "norm_geophone_data = normalize_data(geophone_data, method='max')\n",
    "norm_das_data = normalize_data(das_data, method='max')\n",
    "\n",
    "# Plot normalized data\n",
    "plot_seismic_gather(norm_geophone_data, title=\"Normalized Geophone Data\", xlabel=\"Time Sample\", ylabel=\"Channel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seismic_gather(norm_das_data, title=\"Normalized DAS Data\", xlabel=\"Time Sample\", ylabel=\"Channel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Masked Dataset\n",
    "\n",
    "Now let's create a masked dataset to simulate missing geophone channels. This is the core of our interpolation task - we'll train the model to reconstruct these missing channels using the available geophone data and DAS constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a masked dataset with different patterns\n",
    "# Try different masking patterns\n",
    "mask_patterns = ['random', 'regular', 'block']\n",
    "mask_ratio = 0.3  # 30% of channels masked\n",
    "\n",
    "for pattern in mask_patterns:\n",
    "    # Create masked dataset\n",
    "    masked_geo_data, mask, target_geo_data = create_masked_dataset(\n",
    "        norm_geophone_data, norm_das_data, \n",
    "        mask_pattern=pattern, \n",
    "        mask_ratio=mask_ratio\n",
    "    )\n",
    "    \n",
    "    # Plot the masked data\n",
    "    plot_masked_gather(\n",
    "        norm_geophone_data, \n",
    "        mask, \n",
    "        title=f\"Masked Geophone Data ({pattern} pattern, {int(mask_ratio*100)}% masked)\", \n",
    "        cmap='seismic'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Windowed Data for Training\n",
    "\n",
    "For training, we'll create windowed samples of fixed size from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create windowed datasets\n",
    "window_size = 256  # Number of time steps per window\n",
    "stride = 128  # Stride between windows\n",
    "\n",
    "# Prepare windowed data\n",
    "geophone_windows, das_windows = prepare_dataset_for_training(\n",
    "    times, norm_geophone_data, norm_das_data, \n",
    "    window_size=window_size, \n",
    "    stride=stride\n",
    ")\n",
    "\n",
    "print(f\"Created {geophone_windows.shape[0]} windows of size {window_size}\")\n",
    "print(f\"Geophone windows shape: {geophone_windows.shape}\")\n",
    "print(f\"DAS windows shape: {das_windows.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example windowed data sample\n",
    "sample_idx = 5  # Example window index\n",
    "\n",
    "# Plot geophone window\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(geophone_windows[sample_idx], aspect='auto', cmap='seismic')\n",
    "plt.title(f\"Geophone Window {sample_idx}\")\n",
    "plt.xlabel(\"Time Sample\")\n",
    "plt.ylabel(\"Channel\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot DAS window\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(das_windows[sample_idx], aspect='auto', cmap='seismic')\n",
    "plt.title(f\"DAS Window {sample_idx}\")\n",
    "plt.xlabel(\"Time Sample\")\n",
    "plt.ylabel(\"Channel\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training, Validation, and Test Sets\n",
    "\n",
    "Now let's split the windowed data into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "split_ratio = (0.7, 0.15, 0.15)  # 70% train, 15% validation, 15% test\n",
    "train_data, val_data, test_data = split_dataset(geophone_windows, das_windows, split_ratio=split_ratio)\n",
    "\n",
    "# Unpack the data\n",
    "train_geo, train_das = train_data\n",
    "val_geo, val_das = val_data\n",
    "test_geo, test_das = test_data\n",
    "\n",
    "print(f\"Training set: {train_geo.shape[0]} samples\")\n",
    "print(f\"Validation set: {val_geo.shape[0]} samples\")\n",
    "print(f\"Test set: {test_geo.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Datasets and DataLoaders\n",
    "\n",
    "Let's create PyTorch datasets and dataloaders for both the standard model and the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "batch_size = 32\n",
    "mask_ratio = 0.3\n",
    "mask_pattern = 'random'\n",
    "\n",
    "# Standard dataset for multimodal model\n",
    "train_dataset = SeismicDataset(train_geo, train_das, mask_ratio=mask_ratio, mask_pattern=mask_pattern)\n",
    "val_dataset = SeismicDataset(val_geo, val_das, mask_ratio=mask_ratio, mask_pattern=mask_pattern)\n",
    "test_dataset = SeismicDataset(test_geo, test_das, mask_ratio=mask_ratio, mask_pattern=mask_pattern)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Created DataLoaders with batch size {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer dataset\n",
    "train_transformer_dataset = TransformerSeismicDataset(\n",
    "    train_geo, train_das, \n",
    "    mask_ratio=mask_ratio, \n",
    "    mask_pattern=mask_pattern, \n",
    "    positional_encoding=True\n",
    ")\n",
    "\n",
    "val_transformer_dataset = TransformerSeismicDataset(\n",
    "    val_geo, val_das, \n",
    "    mask_ratio=mask_ratio, \n",
    "    mask_pattern=mask_pattern, \n",
    "    positional_encoding=True\n",
    ")\n",
    "\n",
    "test_transformer_dataset = TransformerSeismicDataset(\n",
    "    test_geo, test_das, \n",
    "    mask_ratio=mask_ratio, \n",
    "    mask_pattern=mask_pattern, \n",
    "    positional_encoding=True\n",
    ")\n",
    "\n",
    "# Create transformer dataloaders\n",
    "train_transformer_loader = DataLoader(train_transformer_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_transformer_loader = DataLoader(val_transformer_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_transformer_loader = DataLoader(test_transformer_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Created Transformer DataLoaders with batch size {batch_size}\")\n",
    "print(f\"Training batches: {len(train_transformer_loader)}\")\n",
    "print(f\"Validation batches: {len(val_transformer_loader)}\")\n",
    "print(f\"Test batches: {len(test_transformer_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine a Batch from the Dataset\n",
    "\n",
    "Let's examine a batch from the dataset to verify it's structured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from the standard dataset\n",
    "for batch in train_loader:\n",
    "    masked_geophone, das, mask, target_geophone = batch\n",
    "    \n",
    "    print(f\"Masked Geophone shape: {masked_geophone.shape}\")\n",
    "    print(f\"DAS shape: {das.shape}\")\n",
    "    print(f\"Mask shape: {mask.shape}\")\n",
    "    print(f\"Target Geophone shape: {target_geophone.shape}\")\n",
    "    \n",
    "    # Count masked channels in first sample\n",
    "    masked_count = mask[0].sum().item()\n",
    "    print(f\"Sample 0 has {masked_count} masked channels out of {mask[0].shape[0]} ({masked_count/mask[0].shape[0]*100:.1f}%)\")\n",
    "    \n",
    "    # Visualize first sample in batch\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.imshow(masked_geophone[0].cpu().numpy(), aspect='auto', cmap='seismic')\n",
    "    plt.title(\"Masked Geophone (Input)\")\n",
    "    plt.ylabel(\"Channel\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.imshow(das[0].cpu().numpy(), aspect='auto', cmap='seismic')\n",
    "    plt.title(\"DAS Data (Input)\")\n",
    "    plt.ylabel(\"Channel\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.imshow(target_geophone[0].cpu().numpy(), aspect='auto', cmap='seismic')\n",
    "    plt.title(\"Target Geophone (Complete)\")\n",
    "    plt.xlabel(\"Time Sample\")\n",
    "    plt.ylabel(\"Channel\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from the transformer dataset\n",
    "for batch in train_transformer_loader:\n",
    "    input_data, attention_mask, positions, target = batch\n",
    "    \n",
    "    print(f\"Input Data shape: {input_data.shape}\")\n",
    "    print(f\"Attention Mask shape: {attention_mask.shape}\")\n",
    "    print(f\"Positions shape: {positions.shape}\")\n",
    "    print(f\"Target shape: {target.shape}\")\n",
    "    \n",
    "    # Visualize first sample in batch\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.imshow(input_data[0].cpu().numpy(), aspect='auto', cmap='seismic')\n",
    "    plt.title(\"Input Data (Concatenated DAS + Masked Geophone)\")\n",
    "    plt.ylabel(\"Channel\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.imshow(target[0].cpu().numpy(), aspect='auto', cmap='seismic')\n",
    "    plt.title(\"Target (Complete Geophone)\")\n",
    "    plt.xlabel(\"Time Sample\")\n",
    "    plt.ylabel(\"Channel\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize attention mask\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(attention_mask[0].cpu().numpy().reshape(1, -1), aspect='auto', cmap='binary')\n",
    "    plt.title(\"Attention Mask (1 = attend, 0 = ignore)\")\n",
    "    plt.xlabel(\"Channel\")\n",
    "    plt.yticks([])\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Datasets for Training\n",
    "\n",
    "Finally, let's save the processed datasets for subsequent model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data\n",
    "processed_dir = Path(\"../data/synthetic/processed/datasets\")\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save windowed data\n",
    "np.save(processed_dir / \"geophone_windows.npy\", geophone_windows)\n",
    "np.save(processed_dir / \"das_windows.npy\", das_windows)\n",
    "\n",
    "# Save train/val/test split indices\n",
    "n_samples = geophone_windows.shape[0]\n",
    "indices = np.random.permutation(n_samples)\n",
    "train_end = int(split_ratio[0] * n_samples)\n",
    "val_end = train_end + int(split_ratio[1] * n_samples)\n",
    "train_indices = indices[:train_end]\n",
    "val_indices = indices[train_end:val_end]\n",
    "test_indices = indices[val_end:]\n",
    "\n",
    "np.save(processed_dir / \"train_indices.npy\", train_indices)\n",
    "np.save(processed_dir / \"val_indices.npy\", val_indices)\n",
    "np.save(processed_dir / \"test_indices.npy\", test_indices)\n",
    "\n",
    "# Save dataset parameters\n",
    "dataset_params = {\n",
    "    \"window_size\": window_size,\n",
    "    \"stride\": stride,\n",
    "    \"n_samples\": n_samples,\n",
    "    \"n_geophone_channels\": geophone_windows.shape[1],\n",
    "    \"n_das_channels\": das_windows.shape[1],\n",
    "    \"mask_ratio\": mask_ratio,\n",
    "    \"split_ratio\": split_ratio,\n",
    "    \"metadata\": metadata\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "with open(processed_dir / \"dataset_params.json\", \"w\") as f:\n",
    "    json.dump(dataset_params, f, indent=2)\n",
    "\n",
    "print(f\"Saved processed datasets to {processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Loaded the combined geophone and DAS dataset\n",
    "2. Preprocessed and normalized the data\n",
    "3. Created masked datasets for training (simulating missing geophone channels)\n",
    "4. Prepared windowed data samples for the model\n",
    "5. Split the data into training, validation, and test sets\n",
    "6. Created PyTorch datasets and dataloaders for both standard and transformer models\n",
    "7. Saved the processed datasets for subsequent model training\n",
    "\n",
    "The data is now ready for training the seismic interpolation model, which we'll do in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
