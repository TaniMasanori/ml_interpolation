{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Seismic Interpolation Model\n",
    "\n",
    "This notebook demonstrates how to train the transformer-based multimodal model for seismic interpolation. We'll go through the following steps:\n",
    "\n",
    "1. Load the preprocessed datasets\n",
    "2. Initialize the model architecture\n",
    "3. Set up training parameters and MLflow tracking\n",
    "4. Train the model\n",
    "5. Evaluate the model's performance\n",
    "6. Save the trained model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Add the project root to path for imports\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.models.transformer import MultimodalSeismicTransformer, StorSeismicBERTModel\n",
    "from src.preprocessing.dataset import SeismicDataset, TransformerSeismicDataset\n",
    "from src.training.trainer import MultimodalTrainer, TransformerSeismicTrainer\n",
    "from src.utils.logging_utils import setup_logging, log_model_summary, log_dataset_info\n",
    "from src.utils.plot_utils import plot_training_history, plot_gather_comparison, plot_trace_comparison\n",
    "\n",
    "# Set up logging\n",
    "logger = setup_logging(level='INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define paths and parameters for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Paths\n",
    "data_dir = \"../data/synthetic/processed/datasets\"\n",
    "models_dir = \"../experiments/models\"\n",
    "results_dir = \"../experiments/results\"\n",
    "\n",
    "# Ensure directories exist\n",
    "Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(results_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data\n",
    "\n",
    "Load the windowed datasets and split indices prepared in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load windowed data\n",
    "geophone_windows = np.load(Path(data_dir) / \"geophone_windows.npy\")\n",
    "das_windows = np.load(Path(data_dir) / \"das_windows.npy\")\n",
    "\n",
    "# Load split indices\n",
    "train_indices = np.load(Path(data_dir) / \"train_indices.npy\")\n",
    "val_indices = np.load(Path(data_dir) / \"val_indices.npy\")\n",
    "test_indices = np.load(Path(data_dir) / \"test_indices.npy\")\n",
    "\n",
    "# Load dataset parameters\n",
    "with open(Path(data_dir) / \"dataset_params.json\", \"r\") as f:\n",
    "    dataset_params = json.load(f)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Loaded {dataset_params['n_samples']} samples with {dataset_params['n_geophone_channels']} geophone channels and {dataset_params['n_das_channels']} DAS channels\")\n",
    "print(f\"Window size: {dataset_params['window_size']}, Stride: {dataset_params['stride']}\")\n",
    "print(f\"Train/Val/Test split: {len(train_indices)}/{len(val_indices)}/{len(test_indices)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data using indices\n",
    "train_geo = geophone_windows[train_indices]\n",
    "train_das = das_windows[train_indices]\n",
    "val_geo = geophone_windows[val_indices]\n",
    "val_das = das_windows[val_indices]\n",
    "test_geo = geophone_windows[test_indices]\n",
    "test_das = das_windows[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Datasets and DataLoaders\n",
    "\n",
    "Create PyTorch datasets and dataloaders for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dataset parameters\n",
    "batch_size = 32\n",
    "mask_ratio = dataset_params['mask_ratio']\n",
    "mask_pattern = 'random'  # Can be 'random', 'regular', or 'block'\n",
    "\n",
    "# Create PyTorch datasets for the transformer model\n",
    "train_dataset = TransformerSeismicDataset(\n",
    "    train_geo, train_das, \n",
    "    mask_ratio=mask_ratio, \n",
    "    mask_pattern=mask_pattern, \n",
    "    positional_encoding=True\n",
    ")\n",
    "\n",
    "val_dataset = TransformerSeismicDataset(\n",
    "    val_geo, val_das, \n",
    "    mask_ratio=mask_ratio, \n",
    "    mask_pattern=mask_pattern, \n",
    "    positional_encoding=True\n",
    ")\n",
    "\n",
    "test_dataset = TransformerSeismicDataset(\n",
    "    test_geo, test_das, \n",
    "    mask_ratio=mask_ratio, \n",
    "    mask_pattern=mask_pattern, \n",
    "    positional_encoding=True\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Log dataset info\n",
    "log_dataset_info(logger, train_dataset, name=\"Training Dataset\")\n",
    "log_dataset_info(logger, val_dataset, name=\"Validation Dataset\")\n",
    "log_dataset_info(logger, test_dataset, name=\"Test Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model\n",
    "\n",
    "Initialize the transformer-based model for seismic interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model parameters\n",
    "n_geophone_channels = dataset_params['n_geophone_channels']\n",
    "n_das_channels = dataset_params['n_das_channels']\n",
    "time_steps = dataset_params['window_size']\n",
    "d_model = 256  # Model dimension\n",
    "nhead = 8  # Number of attention heads\n",
    "num_encoder_layers = 4  # Number of encoder layers\n",
    "num_decoder_layers = 4  # Number of decoder layers\n",
    "dim_feedforward = 1024  # Feedforward dimension\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "# Initialize the model\n",
    "model = StorSeismicBERTModel(\n",
    "    max_channels=n_geophone_channels + n_das_channels,\n",
    "    time_steps=time_steps,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_encoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Log model summary\n",
    "log_model_summary(logger, model)\n",
    "\n",
    "# Print model architecture\n",
    "print(f\"Model Architecture:\\n{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Training\n",
    "\n",
    "Set up the trainer with optimizer, scheduler, and MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up MLflow\n",
    "experiment_name = \"seismic_interpolation_transformer\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 50\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = TransformerSeismicTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    device=device,\n",
    "    experiment_name=experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Train the model and track the progress using MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the model\n",
    "history = trainer.train(\n",
    "    num_epochs=num_epochs,\n",
    "    save_dir=models_dir,\n",
    "    save_freq=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "Visualize the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "plot_training_history(\n",
    "    history,\n",
    "    title=f\"Training History - {experiment_name}\",\n",
    "    save_path=Path(results_dir) / f\"{experiment_name}_training_history.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Evaluate the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to evaluate model on a batch\n",
    "def evaluate_batch(model, batch, device):\n",
    "    \"\"\"Evaluate model on a batch of data.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Unpack batch\n",
    "        input_data, attention_mask, positions, target = batch\n",
    "        input_data = input_data.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        positions = positions.to(device) if positions is not None else None\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_data, attention_mask=attention_mask, position_ids=positions)\n",
    "        \n",
    "        # Extract geophone predictions\n",
    "        n_das_channels = outputs.shape[1] - target.shape[1]\n",
    "        predicted_geophone = outputs[:, n_das_channels:, :]\n",
    "        \n",
    "        return predicted_geophone.cpu().numpy(), target.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get a test batch\n",
    "test_batch = next(iter(test_loader))\n",
    "\n",
    "# Evaluate on the batch\n",
    "predicted_geophone, target_geophone = evaluate_batch(model, test_batch, device)\n",
    "\n",
    "# Select a sample from the batch\n",
    "sample_idx = 0\n",
    "\n",
    "# Plot comparison of true vs predicted\n",
    "plot_gather_comparison(\n",
    "    target_geophone[sample_idx],\n",
    "    predicted_geophone[sample_idx],\n",
    "    title=\"True vs Predicted Geophone Data\",\n",
    "    save_path=Path(results_dir) / f\"{experiment_name}_gather_comparison.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot comparison of individual traces\n",
    "channel_idx = 5  # Choose a channel to plot\n",
    "\n",
    "plot_trace_comparison(\n",
    "    target_geophone[sample_idx, channel_idx],\n",
    "    predicted_geophone[sample_idx, channel_idx],\n",
    "    times=np.arange(dataset_params['window_size']),\n",
    "    title=f\"True vs Predicted - Sample {sample_idx}, Channel {channel_idx}\",\n",
    "    save_path=Path(results_dir) / f\"{experiment_name}_trace_comparison.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the Full Test Set\n",
    "\n",
    "Compute comprehensive metrics on the full test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import evaluation metrics\n",
    "from src.evaluation.metrics import evaluate_model\n",
    "\n",
    "# Evaluate on the test set\n",
    "metrics = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Print metrics\n",
    "print(\"Test Set Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.6f}\")\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.log_metrics({f\"test_{k}\": v for k, v in metrics.items()})\n",
    "    mlflow.log_param(\"evaluation\", \"test_set\")\n",
    "    mlflow.pytorch.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Baseline\n",
    "\n",
    "Let's implement a simple baseline interpolation method (e.g., linear interpolation between available channels) and compare with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simple linear interpolation baseline\n",
    "def baseline_interpolation(masked_data, mask):\n",
    "    \"\"\"Linear interpolation between available channels.\"\"\"\n",
    "    n_channels, n_time_steps = masked_data.shape\n",
    "    interpolated = masked_data.copy()\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        if mask[i]:  # If channel is masked\n",
    "            # Find nearest unmasked channels before and after\n",
    "            before = None\n",
    "            after = None\n",
    "            \n",
    "            # Look backward\n",
    "            for j in range(i-1, -1, -1):\n",
    "                if not mask[j]:\n",
    "                    before = j\n",
    "                    break\n",
    "                    \n",
    "            # Look forward\n",
    "            for j in range(i+1, n_channels):\n",
    "                if not mask[j]:\n",
    "                    after = j\n",
    "                    break\n",
    "            \n",
    "            # Interpolate based on available channels\n",
    "            if before is not None and after is not None:\n",
    "                # Linear interpolation between before and after\n",
    "                weight_after = (i - before) / (after - before)\n",
    "                weight_before = 1 - weight_after\n",
    "                interpolated[i] = weight_before * masked_data[before] + weight_after * masked_data[after]\n",
    "            elif before is not None:\n",
    "                # Use only before\n",
    "                interpolated[i] = masked_data[before]\n",
    "            elif after is not None:\n",
    "                # Use only after\n",
    "                interpolated[i] = masked_data[after]\n",
    "            else:\n",
    "                # No reference channel, use zeros (or could use mean of all data)\n",
    "                interpolated[i] = np.zeros(n_time_steps)\n",
    "                \n",
    "    return interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply baseline to the test batch\n",
    "input_data, attention_mask, positions, target = test_batch\n",
    "\n",
    "# Extract masked geophone data\n",
    "n_das_channels = input_data.shape[1] - target.shape[1]\n",
    "masked_geophone = input_data[:, n_das_channels:, :].cpu().numpy()\n",
    "\n",
    "# Create mask from attention mask\n",
    "mask = ~attention_mask[:, n_das_channels:].bool().cpu().numpy()\n",
    "\n",
    "# Apply baseline interpolation\n",
    "baseline_predictions = []\n",
    "for i in range(len(masked_geophone)):\n",
    "    baseline_pred = baseline_interpolation(masked_geophone[i], mask[i])\n",
    "    baseline_predictions.append(baseline_pred)\n",
    "baseline_predictions = np.array(baseline_predictions)\n",
    "\n",
    "# Plot comparison\n",
    "sample_idx = 0  # Same sample as before\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(target[sample_idx].numpy(), aspect='auto', cmap='seismic')\n",
    "plt.title(\"True Geophone Data\")\n",
    "plt.xlabel(\"Time Sample\")\n",
    "plt.ylabel(\"Channel\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(baseline_predictions[sample_idx], aspect='auto', cmap='seismic')\n",
    "plt.title(\"Baseline Interpolation\")\n",
    "plt.xlabel(\"Time Sample\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(predicted_geophone[sample_idx], aspect='auto', cmap='seismic')\n",
    "plt.title(\"Transformer Model Prediction\")\n",
    "plt.xlabel(\"Time Sample\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.suptitle(\"Comparison of True Data, Baseline, and Transformer Model\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path(results_dir) / f\"{experiment_name}_baseline_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare metrics\n",
    "from src.evaluation.metrics import mean_squared_error, signal_to_noise_ratio, correlation_coefficient\n",
    "\n",
    "# Calculate metrics for the baseline\n",
    "baseline_mse = mean_squared_error(target[sample_idx].numpy(), baseline_predictions[sample_idx])\n",
    "baseline_snr = signal_to_noise_ratio(target[sample_idx].numpy(), baseline_predictions[sample_idx])\n",
    "baseline_corr = correlation_coefficient(target[sample_idx].numpy().flatten(), baseline_predictions[sample_idx].flatten())\n",
    "\n",
    "# Calculate metrics for the transformer model\n",
    "model_mse = mean_squared_error(target[sample_idx].numpy(), predicted_geophone[sample_idx])\n",
    "model_snr = signal_to_noise_ratio(target[sample_idx].numpy(), predicted_geophone[sample_idx])\n",
    "model_corr = correlation_coefficient(target[sample_idx].numpy().flatten(), predicted_geophone[sample_idx].flatten())\n",
    "\n",
    "# Print comparison\n",
    "print(\"Metrics Comparison for Sample 0:\")\n",
    "print(f\"  Baseline MSE: {baseline_mse:.6f}, Transformer MSE: {model_mse:.6f}, Improvement: {(1 - model_mse/baseline_mse)*100:.2f}%\")\n",
    "print(f\"  Baseline SNR: {baseline_snr:.2f} dB, Transformer SNR: {model_snr:.2f} dB, Improvement: {model_snr - baseline_snr:.2f} dB\")\n",
    "print(f\"  Baseline Correlation: {baseline_corr:.4f}, Transformer Correlation: {model_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Model\n",
    "\n",
    "Save the trained model for later use in inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the final model\n",
    "model_path = Path(models_dir) / f\"{experiment_name}_final_model.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'max_channels': n_geophone_channels + n_das_channels,\n",
    "        'time_steps': time_steps,\n",
    "        'd_model': d_model,\n",
    "        'nhead': nhead,\n",
    "        'num_layers': num_encoder_layers,\n",
    "        'dim_feedforward': dim_feedforward,\n",
    "        'dropout': dropout\n",
    "    },\n",
    "    'dataset_params': dataset_params,\n",
    "    'metrics': metrics\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Saved final model to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Loaded and preprocessed the geophone and DAS data\n",
    "2. Initialized a transformer-based model for seismic interpolation\n",
    "3. Trained the model using MLflow to track experiments\n",
    "4. Evaluated the model's performance on the test set\n",
    "5. Compared with a baseline interpolation method\n",
    "6. Saved the trained model for later use\n",
    "\n",
    "The transformer-based model successfully learned to interpolate missing geophone channels using DAS constraints, outperforming the simple baseline interpolation method. The results demonstrate the effectiveness of the approach for seismic data interpolation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}